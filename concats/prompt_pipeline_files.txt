=== src/services/vector/search.py ===
from typing import List, Dict, Any, Optional
from pinecone import Pinecone
import openai
from functools import lru_cache
import logging
from src.core.config.service_config import get_service_config
from src.api.middleware.error_handler import APIError
import json

logger = logging.getLogger(__name__)

class VectorSearchService:
    def __init__(self, config=None):
        self.config = config or get_service_config()
        self.pc = Pinecone(
            api_key=self.config.PINECONE_API_KEY,
            environment=self.config.PINECONE_ENV
        )
        self._init_clients()
    
    def _init_clients(self):
        """Initialize OpenAI client"""
        try:
            self.openai_client = openai.OpenAI(api_key=self.config.OPENAI_API_KEY)
        except Exception as e:
            logger.error(f"Failed to initialize vector search clients: {str(e)}")
            raise APIError(
                message="Failed to initialize vector search service",
                code="VECTOR_INIT_ERROR"
            )
    
    @lru_cache(maxsize=1000)
    async def get_embedding(self, text: str) -> List[float]:
        """Get embedding for text with caching"""
        try:
            response = self.openai_client.embeddings.create(
                model=self.config.EMBED_MODEL,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Failed to get embedding: {str(e)}")
            raise APIError(
                message="Failed to generate embedding",
                code="EMBEDDING_ERROR"
            )
    
    async def search(self, query: str, index_name: str, filter: Dict[str, Any], top_k: int = 5) -> Dict[str, Any]:
        """Search vector store with proper query extraction"""
        try:
            # Extract search terms using LLM
            search_terms = await self._extract_search_terms_llm(query)
            logger.info(f"Extracted search terms: {search_terms}")
            
            results = {}
            for search_type in ["profile", "job_history"]:
                # Generate specific search query for each type
                type_query = await self._generate_type_specific_query(
                    search_terms, 
                    search_type
                )
                
                # Generate embedding for the refined query
                embedding = await self.get_embedding(type_query)
                
                # Search with type-specific filter
                type_filter = {**filter, "type": search_type}
                matches = await self._search_index(
                    index_name=index_name,
                    vector=embedding,
                    filter=type_filter,
                    top_k=top_k
                )
                results[search_type] = matches
                logger.info(f"Found {len(matches)} matches for {search_type}")
            
            return results
            
        except Exception as e:
            logger.error(f"Search failed: {str(e)}")
            raise APIError(message=f"Vector search failed: {str(e)}", code="SEARCH_ERROR")

    async def _extract_search_terms_llm(self, query: str) -> Dict[str, List[str]]:
        """Use LLM to extract relevant search terms from query"""
        try:
            messages = [
                {"role": "system", "content": """Extract key search terms from the RFP query and return them in JSON format.
                Categorize them into:
                - skills_required: Technical skills and expertise needed
                - project_type: Type of project or domain
                - deliverables: Expected outputs or deliverables
                - constraints: Budget, timeline, or other constraints"""},
                {"role": "user", "content": f"""Please analyze this RFP and extract key search terms. Return your response as a JSON object with the following structure:
{{
    "skills_required": ["skill1", "skill2"],
    "project_type": ["type1", "type2"],
    "deliverables": ["deliverable1", "deliverable2"],
    "constraints": ["constraint1", "constraint2"]
}}

RFP: {query}"""}
            ]
            
            completion = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            
            return json.loads(completion.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Failed to extract search terms: {e}")
            # Fallback to basic extraction
            return {
                "skills_required": query.split(),
                "project_type": [],
                "deliverables": [],
                "constraints": []
            }

    async def _generate_type_specific_query(self, search_terms: Dict[str, List[str]], search_type: str) -> str:
        """Generate optimized search query based on type"""
        if search_type == "profile":
            # Focus on skills and expertise for profile matching
            query_parts = [
                *search_terms["skills_required"],
                *search_terms["project_type"]
            ]
        else:  # job_history
            # Focus on project type and deliverables for past work
            query_parts = [
                *search_terms["project_type"],
                *search_terms["deliverables"],
                *search_terms["constraints"]
            ]
        
        return " ".join(query_parts)

    async def _search_index(self, index_name: str, vector: List[float], filter: Dict[str, Any], top_k: int) -> List[Dict[str, Any]]:
        """Perform the actual vector search"""
        try:
            index = self.pc.Index(index_name)
            results = index.query(
                vector=vector,
                filter=filter,
                top_k=top_k,
                include_metadata=True
            )
            
            matches = [{
                "id": match.id,
                "score": match.score,
                "metadata": match.metadata if hasattr(match, 'metadata') else {}
            } for match in results.matches]
            
            return matches
            
        except Exception as e:
            logger.error(f"Index search failed: {e}")
            return []
    
    async def check_index_stats(self, index_name: str) -> Dict[str, Any]:
        """Get statistics about the index"""
        try:
            index = self.pc.Index(index_name)
            stats = index.describe_index_stats()
            
            # Get sample vector to check metadata
            if stats.get('total_vector_count', 0) > 0:
                sample = index.fetch(ids=['1'])  # Fetch first vector
                if sample and sample.vectors:
                    first_vector = next(iter(sample.vectors.values()))
                    logger.info(f"Sample vector metadata: {first_vector.metadata}")
            
            logger.info(f"Index stats: {stats}")
            return stats
        except Exception as e:
            logger.error(f"Failed to get index stats: {str(e)}")
            return {"error": str(e)} 

=== src/services/content/generator.py ===
from typing import Dict, Any, List, Optional
import openai
import logging
from pydantic import BaseModel
from src.core.config.service_config import ServiceConfig
from src.services.vector.search import VectorSearchService
from src.api.middleware.error_handler import APIError
import json
from src.core.config.prompt_templates import SYSTEM_PROMPTS

logger = logging.getLogger(__name__)

RFP_STRUCTURE = {
    "proposal": """{
    "summary": {
        "job_understanding": "string",
        "proposed_approach": "string",
        "timeline": "string",
        "budget": "string"
    },
    "qualifications": {
        "relevant_experience": ["string"],
        "similar_projects": ["string"],
        "technical_skills": ["string"]
    },
    "methodology": {
        "project_phases": [{
            "phase": "string",
            "deliverables": ["string"],
            "timeline": "string"
        }],
        "tools_technologies": ["string"],
        "quality_assurance": ["string"]
    },
    "value_proposition": {
        "unique_benefits": ["string"],
        "risk_mitigation": ["string"],
        "success_metrics": ["string"]
    }
}"""
}

class ContentRequest(BaseModel):
    message: str
    context_type: str = "job"  # or "profile"
    max_context_items: int = 5

class ContentResponse(BaseModel):
    response: str  # Analysis for left column
    sources: List[Dict[str, Any]]
    confidence: float
    proposal: Optional[str] = None  # Clean proposal text for right column
    suggested_tools: List[str] = []

class ContentGenerationService:
    def __init__(self, vector_service=None, config=None):
        self.config = config or get_service_config()
        self.vector_service = vector_service
        self.openai_client = openai.OpenAI(api_key=self.config.OPENAI_API_KEY)
    
    async def _get_context(self, request: ContentRequest) -> Dict[str, List[Dict[str, Any]]]:
        """Retrieve and organize relevant context from vector store"""
        try:
            # Get search results for both profiles and job history
            search_results = await self.vector_service.search(
                query=request.message,
                index_name=self.config.INDEX_NAME,
                filter={},  # No initial filter - search service handles types
                top_k=request.max_context_items
            )
            
            logger.info(f"Retrieved context: {len(search_results.get('profile', []))} profiles, {len(search_results.get('job_history', []))} job histories")
            return search_results
            
        except Exception as e:
            logger.error(f"Failed to get context: {str(e)}", exc_info=True)
            raise APIError(
                message=f"Failed to retrieve context: {str(e)}",
                code="CONTEXT_ERROR"
            )
    
    def _clean_rfp_text(self, text: str) -> str:
        """Extract and clean the core RFP content from Upwork's full page text"""
        try:
            # Split on common Upwork section markers
            sections = text.split("Skills and Expertise")
            if len(sections) > 1:
                main_content = sections[0]
                
                # Further clean up the main content
                lines = main_content.split('\n')
                cleaned_lines = []
                capture = False
                
                for line in lines:
                    # Skip header metadata
                    if "Posted" in line or "Only freelancers" in line or "Specialized profiles" in line:
                        continue
                        
                    # Start capturing at first non-empty, non-metadata line
                    if not capture and line.strip() and not any(x in line for x in ["Posted", "Only freelancers", "Specialized profiles"]):
                        capture = True
                        
                    if capture:
                        cleaned_line = line.strip()
                        if cleaned_line and not cleaned_line.startswith('•'):
                            cleaned_lines.append(cleaned_line)
                
                # Extract budget
                budget_line = next((line for line in lines if '$' in line), '')
                if budget_line:
                    cleaned_lines.append(f"Budget: {budget_line.strip()}")
                
                return '\n'.join(cleaned_lines)
            return text
        except Exception as e:
            logger.warning(f"Error cleaning RFP text: {e}")
            return text
    
    async def _score_proposal(self, proposal: str, past_proposals: List[Dict[str, Any]], rfp: str) -> Dict[str, Any]:
        """Score a proposal against successful patterns and provide feedback"""
        try:
            messages = [
                {"role": "system", "content": """You are a proposal quality analyzer.
                Carefully compare the given proposal against successful patterns and provide detailed scoring.
                Be specific about what works and what could be improved."""},
                {"role": "user", "content": f"""Please analyze this proposal against successful patterns:

Current Proposal:
{proposal}

RFP:
{rfp}

Past Successful Proposals:
{past_proposals}

Score and provide specific feedback on:
1. Pattern match (1-10): How well it follows successful structural patterns
2. Technical alignment (1-10): Appropriate depth and specificity
3. Experience presentation (1-10): How effectively capabilities are shown
4. Response style (1-10): Tone and approach authenticity
5. Overall authenticity (1-10): How genuine it feels

Return your analysis as JSON with this structure:
{{
    "scores": {{
        "pattern_match": {{ "score": 8, "feedback": "Specific feedback here" }},
        "technical_alignment": {{ "score": 8, "feedback": "Specific feedback here" }},
        "experience_presentation": {{ "score": 8, "feedback": "Specific feedback here" }},
        "response_style": {{ "score": 8, "feedback": "Specific feedback here" }},
        "overall_authenticity": {{ "score": 8, "feedback": "Specific feedback here" }}
    }},
    "average_score": 8.0,
    "improvement_suggestions": [
        "Specific actionable improvements"
    ],
    "strengths": [
        "What worked well"
    ]
}}"""}
            ]

            completion = self.openai_client.chat.completions.create(
                model=self.config.CHAT_MODEL,
                messages=messages,
                temperature=0.7,
                response_format={ "type": "json_object" }
            )
            
            return json.loads(completion.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Failed to score proposal: {str(e)}")
            raise APIError(
                message="Failed to score proposal",
                code="SCORING_ERROR"
            )

    async def _generate_from_context(self, request: ContentRequest, context: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """Generate content using structured context"""
        try:
            cleaned_rfp = self._clean_rfp_text(request.message)
            
            # First attempt at proposal generation
            initial_response = await self._generate_proposal(cleaned_rfp, context)
            logger.info(f"Initial proposal:\n{initial_response['proposal']}")
            
            # Score the proposal
            scores = await self._score_proposal(
                initial_response["proposal"],
                context.get("job_history", []),
                cleaned_rfp
            )
            logger.info(f"Initial scores:\n{json.dumps(scores, indent=2)}")
            
            # If scores are too low, try again with feedback
            if scores["average_score"] < 8.0:
                logger.info(f"Initial proposal scored {scores['average_score']}, attempting improvement")
                logger.info(f"Improvement suggestions:\n{json.dumps(scores['improvement_suggestions'], indent=2)}")
                
                improved_response = await self._generate_proposal(
                    cleaned_rfp,
                    context,
                    previous_attempt={
                        "proposal": initial_response["proposal"],
                        "scores": scores
                    }
                )
                logger.info(f"Improved proposal:\n{improved_response['proposal']}")
                
                # Score again
                improved_scores = await self._score_proposal(
                    improved_response["proposal"],
                    context.get("job_history", []),
                    cleaned_rfp
                )
                logger.info(f"Improved scores:\n{json.dumps(improved_scores, indent=2)}")
                
                return {
                    **improved_response,
                    "scores": improved_scores,
                    "iteration_history": {
                        "initial_scores": scores,
                        "improvement_notes": "Proposal was refined based on initial scoring feedback"
                    }
                }
            
            return {
                **initial_response,
                "scores": scores
            }
            
        except Exception as e:
            logger.error(f"Content generation failed: {str(e)}")
            raise APIError(
                message="Failed to generate content",
                code="CONTENT_GENERATION_ERROR"
            )
    
    async def generate_content(self, request: ContentRequest) -> ContentResponse:
        try:
            # Always get context for RFP requests
            context = await self._get_context(request)
            return await self._generate_from_context(request, context)
        except Exception as e:
            logger.error(f"Content generation failed: {str(e)}")
            raise APIError(
                message="Failed to generate content",
                code="CONTENT_GENERATION_ERROR"
            )

    async def generate_proposal(
        self,
        message: str,
        requirements: Optional[List[str]] = None,
        success_metrics: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Generate a proposal with proper context retrieval and reasoning chain"""
        try:
            # Step 1: Get relevant context from both profiles and job history
            context = await self._get_context(ContentRequest(
                message=message,
                context_type="all",  # Will be handled by search service
                max_context_items=self.config.MAX_CONTEXT_ITEMS
            ))
            
            # Step 2: Analyze context and generate response
            response = await self._generate_from_context(
                request=ContentRequest(message=message),
                context=context
            )
            
            # Step 3: Format response for UI
            result = {
                "message": self._format_reasoning_chain(response),
                "proposal": response.get("proposal", {}),
                "conversation": {
                    "profile_recommendation": response.get("profile_recommendation", ""),
                    "reasoning_steps": response.get("reasoning_steps", []),
                    "suggested_tools": response.get("suggested_tools", [])
                }
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to generate proposal: {str(e)}")
            raise APIError(
                message="Failed to generate proposal",
                code="PROPOSAL_GENERATION_ERROR"
            )

    def _format_reasoning_chain(self, response: Dict[str, Any]) -> str:
        """Format the reasoning chain for conversation display"""
        steps = response.get("reasoning_steps", [])
        if not steps:
            return response.get("message", "")
        
        formatted_steps = [
            f"Step {i+1}: {step['title']}\n{step['details']}"
            for i, step in enumerate(steps)
        ]
        
        return "\n\n".join(formatted_steps)

    async def _generate_proposal(self, rfp: str, context: Dict[str, List[Dict[str, Any]]], previous_attempt: Optional[Dict] = None) -> Dict[str, Any]:
        """Generate a proposal with optional feedback from previous attempt"""
        try:
            # Format context
            profile_context = "\n".join([
                f"Profile {i+1}:\n{item.get('metadata', {}).get('text', '')}"
                for i, item in enumerate(context.get('profile', []))
            ])
            
            job_history_context = "\n".join([
                f"Past Proposal {i+1}:\n{item.get('metadata', {}).get('text', '')}"
                for i, item in enumerate(context.get('job_history', []))
            ])
            
            feedback_prompt = ""
            if previous_attempt:
                feedback_prompt = f"""
Previous attempt scored {previous_attempt['scores']['average_score']}/10.
Key areas for improvement:
{json.dumps(previous_attempt['scores']['improvement_suggestions'], indent=2)}

Previous strengths to maintain:
{json.dumps(previous_attempt['scores']['strengths'], indent=2)}

Revise the proposal keeping these points in mind while maintaining authenticity.
"""

            messages = [
                {"role": "system", "content": """You are a professional proposal writer helping to create Upwork proposals.
                Follow a structured reasoning chain and provide your response in JSON format.
                
                Learning Guidelines:
                - Study the patterns and style from past successful proposals
                - Pay special attention to tone, structure, and technical depth
                - Adapt the successful patterns to match the current RFP's needs
                - Use concrete examples and specific capabilities like past proposals do
                
                Always end proposals with "-Aaron" (e.g., "Best regards, -Aaron" or "Thanks, -Aaron")."""},
                {"role": "user", "content": f"""Please analyze this request and create a proposal following these steps:
                
{feedback_prompt}

1. Study Successful Patterns
   - Analyze structure and flow of past proposals
   - Identify common voice and tone elements
   - Note how technical details are presented
   - Observe typical response lengths and depth

2. Review and Select Profile
   - Compare profiles against RFP requirements
   - Choose profile that best matches needs
   - Note how profile strengths were presented in past proposals

3. Draft Proposal
   - Follow successful structural patterns
   - Adapt voice and tone from past wins
   - Match technical depth appropriately
   - Mirror effective presentation styles

RFP Content:
{rfp}

Available Profiles (Choose One):
{profile_context if profile_context else 'No profile matches found.'}

Past Successful Proposals:
{job_history_context if job_history_context else 'No relevant past proposals found.'}

Return your response as JSON with this structure:
{{
    "reasoning_steps": [
        {{
            "title": "Pattern Analysis",
            "details": "Key patterns from successful proposals"
        }},
        {{
            "title": "Profile Selection",
            "details": "Profile choice and presentation approach"
        }},
        {{
            "title": "Proposal Draft",
            "details": "Following successful patterns"
        }}
    ],
    "message": "Your conversational explanation to the user",
    "proposal": "The complete proposal text",
    "profile_recommendation": "Selected profile with reasoning"
}}"""}
            ]

            completion = self.openai_client.chat.completions.create(
                model=self.config.CHAT_MODEL,
                messages=messages,
                temperature=0.7,
                response_format={ "type": "json_object" }
            )
            
            return json.loads(completion.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Failed to generate proposal: {str(e)}")
            raise APIError(
                message="Failed to generate content",
                code="GENERATION_ERROR"
            ) 

=== src/core/config/prompt_templates.py ===
SYSTEM_PROMPTS = {
    "system": """You are an AI assistant responsible for writing and refining Upwork proposals on behalf of Aaron. You have access to a single vector store that contains:
- Profile data (expertise, achievements, skill sets)
- Past successful proposals (writing style examples, client feedback)
- Job details (RFPs, job descriptions, and outcomes)

Instructions:
1. When drafting a proposal, retrieve voice and structural cues from past successful proposals (type = "job_history").
2. Reference Aaron's profile data (type = "profile") to highlight relevant expertise and achievements that match the RFP.
3. Only include details found in the vector store. If you cannot confirm something from the data, say you do not know.
4. Focus on clarity, conciseness, and professional appeal.
5. Implement Upwork best practices:
   - Address the client's needs directly
   - Highlight specific experience that solves the client's challenges
   - Maintain a confident, friendly, and solution-oriented tone
   - Avoid fabrication of details

Style rules:
- Write in a direct, concise voice
- Do not hedge or qualify
- Provide creative guesses only if consistent with the data
- Stay neutral and never apologize
- Ask clarifying questions if the RFP is unclear

Return your response as a JSON object with the structure:
{
    "conversation_message": string,
    "proposal": {
        "introduction": string,
        "approach": {
            "description": string,
            "key_points": string[]
        },
        "timeline_budget": string,
        "qualifications": string,
        "closing": string
    },
    "analysis": {
        "data_usage": string,
        "style_adherence": string,
        "notes": string
    }
}""",

    "conversation": """Generate a concise, friendly message confirming that you have prepared a proposal based on the vector store data, highlighting how you used past successes and current profile details to match the client's needs."""
}

PROPOSAL_GENERATION_PROMPT = """You are a professional proposal writer helping to create detailed business proposals. Follow this reasoning chain and provide your response in JSON format:

1. Analyze the RFP requirements and context
2. Review provided profile matches
3. Study similar past proposals
4. Synthesize a tailored response

Request: {request}

Available Context:
Profiles:
{profile_context}

Past Proposals:
{proposal_context}

Required JSON Response Format:
{
    "message": "Your conversational explanation to the user",
    "proposal": {
        "summary": {
            "job_understanding": "Brief overview of the project requirements",
            "proposed_approach": "High-level approach to solving the problem",
            "timeline": "Estimated timeline for completion",
            "budget": "Budget considerations and pricing structure"
        },
        "qualifications": {
            "skills": "Required skills and experience",
            "expertise": "Relevant domain expertise"
        },
        "methodology": {
            "approach": "Detailed approach to implementation",
            "tools": "Tools and technologies to be used"
        },
        "value_proposition": {
            "benefits": "Key benefits of the solution",
            "differentiators": "What makes this solution unique"
        }
    },
    "reasoning_steps": [
        {
            "title": "Step title",
            "details": "Step details"
        }
    ],
    "profile_recommendation": "Recommendation for which profile to use",
    "suggested_tools": ["tool1", "tool2"]
}"""


=== src/services/conversation/manager.py ===
from typing import Dict, Any, List, Optional, Literal
from uuid import uuid4
from datetime import datetime, timedelta
import logging
from pydantic import BaseModel
from enum import Enum

from src.core.config.service_config import get_service_config
from src.services.conversation.state import (
    ConversationState,
    MessageType,
    ProposalContext,
    ConversationService
)
from src.utils.errors import APIError
from src.services.conversation.async_state import AsyncConversationService
from src.services.vector.search import VectorSearchService
from src.services.content.generator import ContentGenerationService
from src.services.llm import LLMService

logger = logging.getLogger(__name__)

def get_content_service():
    """Get or create content service instance"""
    config = get_service_config()
    vector_service = VectorSearchService(config)
    return ContentGenerationService(vector_service=vector_service, config=config)

class ErrorCode(Enum):
    INTERNAL_ERROR = "internal_error"
    INVALID_REQUEST = "invalid_request"
    NOT_FOUND = "not_found"
    UNAUTHORIZED = "unauthorized"

class ProposalRequest(BaseModel):
    message: str
    type: Literal['proposal_request', 'chat_message']
    requirements: Optional[List[str]] = []
    success_metrics: Optional[Dict[str, Any]] = {}

class ConversationManager:
    def __init__(self, content_service=None, config=None):
        self.config = config or get_service_config()
        self.conversations: Dict[str, ConversationService] = {}
        self.cleanup_interval = timedelta(hours=24)
        self.content_service = content_service or get_content_service()
        self.conversation = AsyncConversationService()
        self.llm_service = LLMService()
    
    async def create_conversation(
        self,
        rfp_text: str,
        requirements: List[str],
        success_metrics: Optional[Dict[str, Any]] = None
    ) -> str:
        """Create a new proposal conversation"""
        try:
            conversation_id = str(uuid4())
            
            # Initialize context with RFP details
            context = ProposalContext(
                rfp_details={"text": rfp_text},
                matched_profiles=[],
                past_proposals=[],
                requirements=requirements,
                success_metrics=success_metrics or {}
            )
            
            conversation = ConversationService()
            conversation.set_context(context)
            
            self.conversations[conversation_id] = conversation
            logger.info(f"Created new proposal conversation: {conversation_id}")
            
            return conversation_id
            
        except Exception as e:
            logger.error(f"Failed to create conversation: {str(e)}")
            raise APIError(message="Failed to create conversation")
    
    async def process_step(self, conversation_id: str) -> Dict[str, Any]:
        """Process the next step in the proposal generation"""
        conversation = await self.get_conversation(conversation_id)
        
        try:
            if conversation.state == ConversationState.INITIAL:
                return await self._analyze_rfp(conversation)
                
            elif conversation.state == ConversationState.ANALYZING:
                return await self._find_matches(conversation)
                
            elif conversation.state == ConversationState.RESEARCHING:
                return await self._draft_proposal(conversation)
                
            elif conversation.state == ConversationState.DRAFTING:
                return await self._review_proposal(conversation)
                
            else:
                raise APIError(message="Invalid conversation state")
                
        except Exception as e:
            logger.error(f"Error processing step: {str(e)}")
            raise APIError(message=f"Failed to process step: {str(e)}")

    async def _analyze_rfp(self, conversation: ConversationService) -> Dict[str, Any]:
        """Analyze RFP and extract key details"""
        context = conversation.get_context()
        rfp_text = context.rfp_details.get("text", "")
        
        # Search for similar RFPs
        search_results = await self.vector_service.search(
            query=rfp_text,
            filter={"type": "rfp"},
            top_k=self.config.MAX_CONTEXT_ITEMS
        )
        
        # Update metrics
        conversation.update_metrics({
            "analysis_complete": True,
            "similar_rfps": len(search_results)
        })
        
        # Add analysis message
        conversation.add_message(
            content="RFP analysis complete",
            msg_type=MessageType.ANALYSIS,
            metadata={"search_results": search_results}
        )
        
        return {"status": "analyzing", "similar_rfps": len(search_results)}

    async def _find_matches(self, conversation: ConversationService) -> Dict[str, Any]:
        """Find matching profiles and past proposals"""
        context = conversation.get_context()
        
        # Search for matching profiles
        profile_matches = await self.vector_service.search(
            query=context.rfp_details["text"],
            filter={"type": "profile"},
            top_k=self.config.MAX_CONTEXT_ITEMS
        )
        
        # Search for relevant past proposals
        past_proposals = await self.vector_service.search(
            query=context.rfp_details["text"],
            filter={"type": "proposal"},
            top_k=self.config.MAX_CONTEXT_ITEMS
        )
        
        # Update context and metrics
        context.matched_profiles = profile_matches
        context.past_proposals = past_proposals
        conversation.update_metrics({
            "profile_matches": len(profile_matches),
            "past_proposals": len(past_proposals)
        })
        
        # Add research message
        conversation.add_message(
            content="Found relevant matches",
            msg_type=MessageType.RESEARCH,
            metadata={
                "profile_matches": len(profile_matches),
                "past_proposals": len(past_proposals)
            }
        )
        
        return {
            "status": "researching",
            "matches": len(profile_matches),
            "past_proposals": len(past_proposals)
        }

    async def _draft_proposal(self, conversation: ConversationService) -> Dict[str, Any]:
        """Generate the proposal draft"""
        context = conversation.get_context()
        
        # Generate proposal using content service
        proposal = await self.content_service.generate_content({
            "rfp": context.rfp_details,
            "profiles": context.matched_profiles,
            "past_proposals": context.past_proposals,
            "requirements": context.requirements
        })
        
        # Add proposal message
        conversation.add_message(
            content="Generated proposal draft",
            msg_type=MessageType.PROPOSAL,
            metadata={"proposal": proposal}
        )
        
        return {"status": "drafting", "proposal": proposal}

    async def get_conversation(self, conversation_id: str) -> ConversationService:
        """Retrieve a conversation by ID"""
        conversation = self.conversations.get(conversation_id)
        if not conversation:
            raise APIError(
                message=f"Conversation not found: {conversation_id}",
                status_code=404
            )
        return conversation

    def cleanup_old_conversations(self):
        """Remove conversations older than cleanup_interval"""
        current_time = datetime.now()
        to_remove = []
        
        for conv_id, conv in self.conversations.items():
            if current_time - conv.messages[-1].timestamp > self.cleanup_interval:
                to_remove.append(conv_id)
        
        for conv_id in to_remove:
            del self.conversations[conv_id]
            logger.info(f"Cleaned up old conversation: {conv_id}")
    
    async def get_conversation_summary(self, conversation_id: str) -> Dict[str, Any]:
        """Get a summary of the conversation"""
        conversation = await self.get_conversation(conversation_id)
        return conversation.get_conversation_summary()
    
    async def get_active_conversations(self) -> List[Dict[str, Any]]:
        """Get summaries of all active conversations"""
        return [
            conv.get_conversation_summary()
            for conv in self.conversations.values()
        ]

    async def process_request(self, request: ProposalRequest):
        """Process a chat message and generate proposal content"""
        try:
            # Use the content service to generate response
            response = await self.content_service.generate_proposal(
                message=request.message,
                requirements=request.requirements,
                success_metrics=request.success_metrics
            )
            
            # Add the response to conversation history
            if response.get("message"):
                await self.conversation.add_message(
                    content=response["message"],
                    msg_type=MessageType.AGENT_MESSAGE
                )
            
            # Handle string or dict proposal format
            proposal = response.get("proposal", {})
            if isinstance(proposal, str):
                proposal = {"content": proposal}  # Wrap string in dict to satisfy type checking
            
            return {
                "response": response.get("message"),
                "proposal": proposal,
                "confidence": response.get("confidence", 0.0),
                "sources": response.get("sources", [])
            }
            
        except Exception as e:
            logger.error(f"Error processing request: {e}")
            raise APIError(
                message="Failed to process request",
                code="PROCESSING_ERROR"
            ) 

=== src/services/conversation/async_state.py ===
import logging  # Add standard logging
from dataclasses import dataclass  # Add this import
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, Any, Union, TypedDict, Literal
from pydantic import BaseModel
from uuid import uuid4  # Add this import
from src.utils.errors import APIError, ErrorCode
from .state import MessageType, ConversationState

logger = logging.getLogger(__name__)  # Use standard logging

class ConversationState(Enum):
    INITIAL = "INITIAL"
    ANALYZING = "ANALYZING"
    RESEARCHING = "RESEARCHING"
    DRAFTING = "DRAFTING"
    REVIEWING = "REVIEWING"
    COMPLETE = "COMPLETE"

class MessageType(Enum):
    TEXT = "TEXT"
    USER_MESSAGE = "user_message"
    AGENT_MESSAGE = "agent_message"
    SYSTEM_MESSAGE = "system_message"
    RFP = "rfp"
    REASONING_STEP = "reasoning_step"
    PROPOSAL = "proposal"

class ClientDetails(BaseModel):
    location: str
    payment_verified: bool
    rating: Optional[str]
    spent: Optional[str]

class RFPContent(BaseModel):
    title: str
    description: str
    budget_type: Literal['fixed', 'hourly']
    budget_amount: str
    skills: List[str]
    experience_level: str
    location_requirements: str
    project_duration: str
    client_details: ClientDetails

class ProposalContext(BaseModel):
    rfp_details: Dict[str, Any]
    matched_profiles: List[Dict[str, Any]]
    past_proposals: List[Dict[str, Any]]
    requirements: List[str]
    success_metrics: Dict[str, Any]

class ReasoningStepMetadata(BaseModel):
    step_type: Literal['analysis', 'research', 'generation']
    confidence_score: Optional[float] = None
    matches_found: Optional[int] = None
    time_taken: Optional[int] = None

@dataclass
class Message:
    id: str
    type: MessageType
    content: Union[str, RFPContent]
    sender: Literal['user', 'agent']
    timestamp: datetime
    metadata: Optional[ReasoningStepMetadata] = None

class AsyncConversationService:
    def __init__(self):
        self.messages: List[Dict[str, Any]] = []
        self.state = ConversationState.INITIAL
        self.context: Dict[str, Any] = {}
        self._transitions = {
            ConversationState.INITIAL: [ConversationState.ANALYZING],
            ConversationState.ANALYZING: [ConversationState.RESEARCHING],
            ConversationState.RESEARCHING: [ConversationState.DRAFTING],
            ConversationState.DRAFTING: [ConversationState.REVIEWING],
            ConversationState.REVIEWING: [ConversationState.COMPLETE, ConversationState.DRAFTING],
            ConversationState.COMPLETE: [ConversationState.INITIAL]
        }
        self.metrics: Dict[str, Any] = {
            "profile_matches": [],
            "past_proposals": [],
            "confidence_score": 0.0,
            "analysis_complete": False,
            "state_timestamps": {}
        }
        logger.info("Initialized AsyncConversationService")

    async def add_message(
        self,
        content: str,
        msg_type: MessageType,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """Add a message to the conversation"""
        try:
            message = {
                "content": content,
                "type": msg_type,
                "metadata": metadata or {},
                "timestamp": datetime.now().timestamp()
            }
            self.messages.append(message)
            await self._update_state(message)
        except Exception as e:
            logger.error(f"Error adding message: {e}")
            raise APIError(message=f"Failed to add message: {str(e)}")

    async def transition_to(self, state: ConversationState):
        """Update conversation state"""
        try:
            if state not in self._transitions[self.state]:
                logger.warning(f"Invalid transition attempted: {self.state} -> {state}")
                raise APIError(
                    code=ErrorCode.INVALID_STATE_TRANSITION,
                    message=f"Invalid transition from {self.state} to {state}"
                )
            
            old_state = self.state
            self.state = state
            
            # Log state transition time
            now = datetime.utcnow()
            self.metrics["state_timestamps"][state] = now
            
            if old_state in self.metrics["state_timestamps"]:
                time_in_state = now - self.metrics["state_timestamps"][old_state]
                logger.info(f"Time spent in {old_state}: {time_in_state.total_seconds()}s")
            
            logger.info(f"State transition: {old_state} -> {state}")
            return True
            
        except KeyError:
            logger.error(f"Invalid state encountered: {self.state}")
            raise APIError(
                code=ErrorCode.INVALID_STATE,
                message=f"Invalid state: {self.state}"
            )

    async def _update_state(self, message: Dict[str, Any]):
        """Update state based on message type and current state"""
        try:
            msg_type = message["type"]
            if isinstance(msg_type, MessageType):
                msg_type = msg_type.value
                
            if msg_type == MessageType.AGENT_MESSAGE.value:
                if self.state == ConversationState.ANALYZING:
                    await self.transition_to(ConversationState.RESEARCHING)
                elif self.state == ConversationState.RESEARCHING:
                    await self.transition_to(ConversationState.GENERATING)
                elif self.state == ConversationState.GENERATING:
                    await self.transition_to(ConversationState.COMPLETE)
        except Exception as e:
            logger.error(f"Error updating state: {e}")
            raise APIError(message=f"Error updating state: {str(e)}")

    # Synchronous methods that don't require I/O
    def get_context(self) -> Optional[ProposalContext]:
        return self.context

    def get_conversation_history(self, limit: int = None) -> List[Dict[str, Any]]:
        if limit:
            return self.messages[-limit:]
        return self.messages

    async def reset(self) -> None:
        """Reset conversation state"""
        self.state = ConversationState.INITIAL
        self.messages = []
        self.context = {}
        self.metrics = {
            "profile_matches": [],
            "past_proposals": [],
            "confidence_score": 0.0,
            "analysis_complete": False,
            "state_timestamps": {}
        }
        logger.info("Conversation state reset")

    async def set_context(self, context: ProposalContext) -> None:
        """Set conversation context with async support for future DB operations"""
        self.context = context
        logger.debug(f"Set context with RFP ID: {context.rfp_details.get('id', 'unknown')}")

    def get_conversation_summary(self) -> Dict[str, Any]:
        """Get a summary of the conversation"""
        return {
            "messages": self.messages,
            "state": self.state.value,
            "context": self.context
        } 

=== frontend/src/components/chat/ProposalDisplay.tsx ===
import React from 'react';
import { Box, Typography, Paper, IconButton, Snackbar, Divider } from '@mui/material';
import ContentCopyIcon from '@mui/icons-material/ContentCopy';
import styles from './ProposalDisplay.module.css';

interface ProposalDisplayProps {
  proposal: string | {
    content?: string;  // Add this for string proposals
    summary?: {
      job_understanding?: string;
      proposed_approach?: string;
      timeline?: string;
      budget?: string;
    };
    qualifications?: Record<string, string>;
    methodology?: Record<string, string>;
    value_proposition?: Record<string, string>;
    scores?: {
      scores?: Record<string, { score: number; feedback: string }>;
      improvement_suggestions?: string[];
    };
    iteration_history?: {
      initial_scores: { average_score: number };
      improvement_notes: string;
    };
  };
}

export const ProposalDisplay: React.FC<ProposalDisplayProps> = ({ proposal }) => {
  const [showCopied, setShowCopied] = React.useState(false);
  const scores = proposal.scores || {};
  const iterationHistory = proposal.iteration_history;

  if (!proposal) {
    return null;
  }

  const handleCopy = async () => {
    const textToCopy = typeof proposal === 'string' 
      ? proposal 
      : proposal.content || Object.entries(proposal.summary || {})
          .map(([key, value]) => value)
          .filter(Boolean)
          .join('\n\n');

    try {
      await navigator.clipboard.writeText(textToCopy);
      setShowCopied(true);
    } catch (err) {
      console.error('Failed to copy text:', err);
    }
  };

  const renderProposal = () => {
    if (typeof proposal === 'string' || proposal.content) {
      const content = typeof proposal === 'string' ? proposal : proposal.content;
      return (
        <Typography 
          variant="body1" 
          component="pre" 
          className={styles.proposalText}
          sx={{ whiteSpace: 'pre-wrap' }}
        >
          {content}
        </Typography>
      );
    }

    // Handle legacy structured format
    return (
      <>
        {proposal.summary?.job_understanding && (
          <>
            <Typography variant="h6" gutterBottom>
              Understanding
            </Typography>
            <Typography variant="body2" paragraph>
              {proposal.summary.job_understanding}
            </Typography>
          </>
        )}

        {proposal.summary?.proposed_approach && (
          <>
            <Typography variant="h6" gutterBottom>
              Approach
            </Typography>
            <Typography variant="body2" paragraph>
              {proposal.summary.proposed_approach}
            </Typography>
          </>
        )}

        {proposal.summary?.timeline && (
          <>
            <Typography variant="h6" gutterBottom>
              Timeline
            </Typography>
            <Typography variant="body2" paragraph>
              {proposal.summary.timeline}
            </Typography>
          </>
        )}

        {proposal.summary?.budget && (
          <>
            <Typography variant="h6" gutterBottom>
              Budget
            </Typography>
            <Typography variant="body2" paragraph>
              {proposal.summary.budget}
            </Typography>
          </>
        )}
      </>
    );
  };

  return (
    <div className={styles.proposalWrapper}>
      <Paper elevation={2} className={styles.proposalSection}>
        <Typography variant="h6" gutterBottom>
          Final Proposal
        </Typography>
        <Typography className={styles.proposalText}>
          {proposal.proposal}
        </Typography>
      </Paper>

      {scores && (
        <Paper elevation={2} className={styles.scoreSection}>
          <Typography variant="h6" gutterBottom>
            Proposal Analysis
          </Typography>
          
          {Object.entries(scores.scores || {}).map(([key, value]: [string, any]) => (
            <Box key={key} className={styles.scoreItem}>
              <Box className={styles.scoreHeader}>
                <Typography variant="subtitle1" className={styles.scoreTitle}>
                  {key.split('_').map(word => word.charAt(0).toUpperCase() + word.slice(1)).join(' ')}
                </Typography>
                <Typography variant="h6" className={styles.scoreValue}>
                  {value.score}/10
                </Typography>
              </Box>
              <Typography variant="body2" color="textSecondary">
                {value.feedback}
              </Typography>
            </Box>
          ))}

          <Divider className={styles.divider} />
          
          <Box className={styles.improvementSection}>
            <Typography variant="subtitle1" gutterBottom>
              Suggested Improvements
            </Typography>
            <ul>
              {scores.improvement_suggestions?.map((suggestion: string, index: number) => (
                <li key={index}>
                  <Typography variant="body2">{suggestion}</Typography>
                </li>
              ))}
            </ul>
          </Box>

          {iterationHistory && (
            <Box className={styles.iterationSection}>
              <Typography variant="subtitle1" gutterBottom>
                Iteration History
              </Typography>
              <Typography variant="body2">
                Initial Score: {iterationHistory.initial_scores.average_score}/10
              </Typography>
              <Typography variant="body2">
                {iterationHistory.improvement_notes}
              </Typography>
            </Box>
          )}
        </Paper>
      )}

      <Paper elevation={1} className={styles.proposalContainer}>
        <Box className={styles.proposalHeader}>
          <IconButton 
            onClick={handleCopy}
            className={styles.copyButton}
            aria-label="Copy to clipboard"
            size="large"
          >
            <ContentCopyIcon />
          </IconButton>
        </Box>
        {renderProposal()}
        <Snackbar
          open={showCopied}
          autoHideDuration={2000}
          onClose={() => setShowCopied(false)}
          message="Copied to clipboard"
          anchorOrigin={{ vertical: 'bottom', horizontal: 'center' }}
        />
      </Paper>
    </div>
  );
}; 

=== src/core/config/service_config.py ===
from typing import Dict, ClassVar
from functools import lru_cache
from pydantic_settings import BaseSettings, SettingsConfigDict
import logging

class ServiceConfig(BaseSettings):
    # OpenAI Configuration
    OPENAI_API_KEY: str
    CHAT_MODEL: str = "gpt-4-turbo-preview"
    EMBED_MODEL: str = "text-embedding-ada-002"
    
    # Pinecone Configuration
    PINECONE_API_KEY: str
    PINECONE_ENV: str
    INDEX_NAME: str = "upwork-v1"
    PINECONE_INDEX_NAME: str = "upwork-v1"  # For backward compatibility
    
    # Vector Search Configuration
    MAX_CONTEXT_ITEMS: int = 5
    SEARCH_TOP_K: int = 5
    CHUNK_SIZE_TOKENS: int = 700
    DEDUP_SIM_THRESHOLD: float = 0.95
    
    # Content Types for metadata
    CONTENT_TYPES: ClassVar[Dict[str, str]] = {
        "profile": "profile",
        "job": "job",
        "job_history": "job_history"
    }
    
    # Optional Rate Limiting
    RATE_LIMIT_CALLS: int = 50
    RATE_LIMIT_PERIOD: int = 60
    
    # Debug settings
    DEBUG: bool = False

    # Use SettingsConfigDict instead of Config class in Pydantic v2
    model_config = SettingsConfigDict(
        env_file=".env",
        extra="allow"  # Allow extra fields from env file
    )

    def setup_logging(self):
        """Configure logging for the service"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('logs/app.log')
            ]
        )
        logging.info("Logging initialized with level: INFO")
        logging.info("Log file: logs/app.log")

@lru_cache()
def get_service_config() -> ServiceConfig:
    config = ServiceConfig()
    config.setup_logging()
    return config 